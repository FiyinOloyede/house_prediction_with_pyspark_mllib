{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OVERVIEW\n",
    "\n",
    "This project analyzes a house sales dataset in King County, USA, to predicts housing prices using Apache Spark’s MLlib library to build different regression models through the PySpark API. The optimal model was chosen based on the least Root Mean Squared Error and highest R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ABOUT DATASET\n",
    "\n",
    "The dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015 and it's a good dataset for evaluating simple regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable      | Description                                                                                                 |\n",
    "| ------------- | ----------------------------------------------------------------------------------------------------------- |\n",
    "| id            | A notation for a house                                                                                      |\n",
    "| date          | Date house was sold                                                                                         |\n",
    "| price         | Price is prediction target                                                                                  |\n",
    "| bedrooms      | Number of bedrooms                                                                                          |\n",
    "| bathrooms     | Number of bathrooms                                                                                         |\n",
    "| sqft_living   | Square footage of the home                                                                                  |\n",
    "| sqft_lot      | Square footage of the lot                                                                                   |\n",
    "| floors        | Total floors (levels) in house                                                                              |\n",
    "| waterfront    | House which has a view to a waterfront                                                                      |\n",
    "| view          | Has been viewed                                                                                             |\n",
    "| condition     | How good the condition is overall                                                                           |\n",
    "| grade         | overall grade given to the housing unit, based on King County grading system                                |\n",
    "| sqft_above    | Square footage of house apart from basement                                                                 |\n",
    "| sqft_basement | Square footage of the basement                                                                              |\n",
    "| yr_built      | Built Year                                                                                                  |\n",
    "| yr_renovated  | Year when house was renovated                                                                               |\n",
    "| zipcode       | Zip code                                                                                                    |\n",
    "| lat           | Latitude coordinate                                                                                         |\n",
    "| long          | Longitude coordinate                                                                                        |\n",
    "| sqft_living15 | Living room area in 2015(implies-- some renovations) This might or might not have affected the lotsize area |\n",
    "| sqft_lot15    | LotSize area in 2015(implies-- some renovations)                                                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETUP\n",
    "\n",
    "We set up Spark environment to initialize SparkSession and import the required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing required libraries\n",
    "# to find spark\n",
    "import findspark # to run PySpark in Jupyter notebook\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "# import spark\n",
    "from pyspark.sql import SparkSession      # To create a Spark session, the entry point for using Spark functionality\n",
    "from pyspark.sql.functions import regexp_replace, col, substring\n",
    "from pyspark.sql.functions import corr    # Calculates the Pearson correlation coefficient between two columns\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor # Provides Regression models\n",
    "from pyspark.ml.feature import VectorAssembler     # Combines multiple columns into a single vector column, required for MLlib models\n",
    "from pyspark.ml.evaluation import RegressionEvaluator # Evaluates the performance of regression models\n",
    "\n",
    "\n",
    "# initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Regression with PySpark MLlib') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATASET\n",
    "\n",
    "Set the header and inferSchema parameters to be True, which means the first line contains the header and automatic detection of the underlying data schema respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = spark.read.csv(\"kc_house_data.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+\n",
      "|        id|           date|   price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|condition|grade|sqft_above|sqft_basement|yr_built|yr_renovated|zipcode|    lat|    long|sqft_living15|sqft_lot15|\n",
      "+----------+---------------+--------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+\n",
      "|7129300520|20141013T000000|221900.0|       3|      1.0|       1180|    5650|   1.0|         0|   0|        3|    7|      1180|            0|    1955|           0|  98178|47.5112|-122.257|         1340|      5650|\n",
      "|6414100192|20141209T000000|538000.0|       3|     2.25|       2570|    7242|   2.0|         0|   0|        3|    7|      2170|          400|    1951|        1991|  98125| 47.721|-122.319|         1690|      7639|\n",
      "|5631500400|20150225T000000|180000.0|       2|      1.0|        770|   10000|   1.0|         0|   0|        3|    6|       770|            0|    1933|           0|  98028|47.7379|-122.233|         2720|      8062|\n",
      "|2487200875|20141209T000000|604000.0|       4|      3.0|       1960|    5000|   1.0|         0|   0|        5|    7|      1050|          910|    1965|           0|  98136|47.5208|-122.393|         1360|      5000|\n",
      "|1954400510|20150218T000000|510000.0|       3|      2.0|       1680|    8080|   1.0|         0|   0|        3|    8|      1680|            0|    1987|           0|  98074|47.6168|-122.045|         1800|      7503|\n",
      "+----------+---------------+--------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the dataset\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that the target variable (price) in the third column. As a good practice, let us rearrange columns and make target variable to be last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+--------+\n",
      "|        id|           date|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|condition|grade|sqft_above|sqft_basement|yr_built|yr_renovated|zipcode|    lat|    long|sqft_living15|sqft_lot15|   price|\n",
      "+----------+---------------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+--------+\n",
      "|7129300520|20141013T000000|       3|      1.0|       1180|    5650|   1.0|         0|   0|        3|    7|      1180|            0|    1955|           0|  98178|47.5112|-122.257|         1340|      5650|221900.0|\n",
      "|6414100192|20141209T000000|       3|     2.25|       2570|    7242|   2.0|         0|   0|        3|    7|      2170|          400|    1951|        1991|  98125| 47.721|-122.319|         1690|      7639|538000.0|\n",
      "|5631500400|20150225T000000|       2|      1.0|        770|   10000|   1.0|         0|   0|        3|    6|       770|            0|    1933|           0|  98028|47.7379|-122.233|         2720|      8062|180000.0|\n",
      "|2487200875|20141209T000000|       4|      3.0|       1960|    5000|   1.0|         0|   0|        5|    7|      1050|          910|    1965|           0|  98136|47.5208|-122.393|         1360|      5000|604000.0|\n",
      "|1954400510|20150218T000000|       3|      2.0|       1680|    8080|   1.0|         0|   0|        3|    8|      1680|            0|    1987|           0|  98074|47.6168|-122.045|         1800|      7503|510000.0|\n",
      "+----------+---------------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rearrange dataset column\n",
    "rearranged_col = [\"id\", \"date\", \"bedrooms\", \"bathrooms\", \"sqft_living\", \"sqft_lot\", \"floors\", \"waterfront\", \"view\", \"condition\", \"grade\", \"sqft_above\", \"sqft_basement\", \"yr_built\",  \"yr_renovated\", \"zipcode\", \"lat\", \"long\", \"sqft_living15\", \"sqft_lot15\", \"price\"]\n",
    "df = df.select(rearranged_col)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA CLEANING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the Types of Data\n",
    "\n",
    "Let us check for the datatype because sometimes variables can be stored in wrong format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- sqft_living: integer (nullable = true)\n",
      " |-- sqft_lot: integer (nullable = true)\n",
      " |-- floors: double (nullable = true)\n",
      " |-- waterfront: integer (nullable = true)\n",
      " |-- view: integer (nullable = true)\n",
      " |-- condition: integer (nullable = true)\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- sqft_above: integer (nullable = true)\n",
      " |-- sqft_basement: integer (nullable = true)\n",
      " |-- yr_built: integer (nullable = true)\n",
      " |-- yr_renovated: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- sqft_living15: integer (nullable = true)\n",
      " |-- sqft_lot15: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check data types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it appears all features as stored correctly so nothing to worry about.\n",
    "\n",
    "Let us move on to dropping irrelevant columns. This step is certainly required because sometimes there would be columns that will not be needed, in such cases dropping is the right thing to do. In this case, the “id” column will not contribute any value to our analysis so we will be dropping it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+--------+\n",
      "|           date|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|condition|grade|sqft_above|sqft_basement|yr_built|yr_renovated|zipcode|    lat|    long|sqft_living15|sqft_lot15|   price|\n",
      "+---------------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+--------+\n",
      "|20141013T000000|       3|      1.0|       1180|    5650|   1.0|         0|   0|        3|    7|      1180|            0|    1955|           0|  98178|47.5112|-122.257|         1340|      5650|221900.0|\n",
      "|20141209T000000|       3|     2.25|       2570|    7242|   2.0|         0|   0|        3|    7|      2170|          400|    1951|        1991|  98125| 47.721|-122.319|         1690|      7639|538000.0|\n",
      "|20150225T000000|       2|      1.0|        770|   10000|   1.0|         0|   0|        3|    6|       770|            0|    1933|           0|  98028|47.7379|-122.233|         2720|      8062|180000.0|\n",
      "|20141209T000000|       4|      3.0|       1960|    5000|   1.0|         0|   0|        5|    7|      1050|          910|    1965|           0|  98136|47.5208|-122.393|         1360|      5000|604000.0|\n",
      "|20150218T000000|       3|      2.0|       1680|    8080|   1.0|         0|   0|        3|    8|      1680|            0|    1987|           0|  98074|47.6168|-122.045|         1800|      7503|510000.0|\n",
      "+---------------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop irrelevant column \n",
    "df = df.drop(\"id\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting Columns for Numerical Analysis\n",
    "\n",
    "Let's extract the first 4 characters of the 'date' column to format it as year this will ensure all our features are in numerical type which is good for our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract the first 4 characters of the 'date' column to format it as year\n",
    "# this will ensure all our features are in numerical type which is good for our analysis\n",
    "df = df.withColumn(\"date\", substring(col(\"date\"), 1, 4).cast(\"int\"))\n",
    "\n",
    "# Rename the 'date' column to 'yr_sold'\n",
    "df = df.withColumnRenamed(\"date\", \"yr_sold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all our features are in numeric let us obtain Statistical summary from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-----------------+------------------+------------------+\n",
      "|summary|           yr_sold|         bedrooms|         bathrooms|       sqft_living|          sqft_lot|            floors|          waterfront|               view|         condition|             grade|        sqft_above|     sqft_basement|          yr_built|      yr_renovated|           zipcode|                lat|               long|    sqft_living15|        sqft_lot15|             price|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-----------------+------------------+------------------+\n",
      "|  count|             21613|            21613|             21613|             21613|             21613|             21613|               21613|              21613|             21613|             21613|             21613|             21613|             21613|             21613|             21613|              21613|              21613|            21613|             21613|             21613|\n",
      "|   mean|2014.3229537778188| 3.37084162309721|2.1147573219821405|2079.8997362698374|15106.967565816869|1.4943089807060566|0.007541757275713691|0.23430342849211122|3.4094295100171195| 7.656873178179799|1788.3906907879516| 291.5090454818859|1971.0051357978994| 84.40225790033776| 98077.93980474715|  47.56005251931704|-122.21389640494083|1986.552491556008|12768.455651691113| 540088.1417665294|\n",
      "| stddev|0.4676160310451533|0.930061831147451| 0.770163157217741| 918.4408970468096| 41420.51151513551|0.5399888951423489| 0.08651719772788748| 0.7663175692736114|0.6507430463662044|1.1754587569743344| 828.0909776519175|442.57504267746685|29.373410802386243|401.67924001917504|53.505026257472466|0.13856371024192368|0.14082834238139288|685.3913042527788|27304.179631338524|367127.19648270035|\n",
      "|    min|              2014|                0|               0.0|               290|               520|               1.0|                   0|                  0|                 1|                 1|               290|                 0|              1900|                 0|             98001|            47.1559|           -122.519|              399|               651|           75000.0|\n",
      "|    max|              2015|               33|               8.0|             13540|           1651359|               3.5|                   1|                  4|                 5|                13|              9410|              4820|              2015|              2015|             98199|            47.7776|           -121.315|             6210|            871200|         7700000.0|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistical summary\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking and Dropping Duplicate Rows\n",
    "\n",
    "This is often a useful thing to do because a huge data set like in this case sometimes have some duplicate data which can impact analysis result, so we remove all the duplicate value from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows before: 4\n",
      "Duplicate rows after: 0\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+------------------+--------------------+-------------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+-----------------+-------------------+-------------------+------------------+------------------+-----------------+\n",
      "|summary|           yr_sold|          bedrooms|         bathrooms|      sqft_living|         sqft_lot|            floors|          waterfront|               view|         condition|            grade|        sqft_above|     sqft_basement|         yr_built|     yr_renovated|          zipcode|                lat|               long|     sqft_living15|        sqft_lot15|            price|\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+------------------+--------------------+-------------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+-----------------+-------------------+-------------------+------------------+------------------+-----------------+\n",
      "|  count|             21609|             21609|             21609|            21609|            21609|             21609|               21609|              21609|             21609|            21609|             21609|             21609|            21609|            21609|            21609|              21609|              21609|             21609|             21609|            21609|\n",
      "|   mean|2014.3229210051368|3.3709102688694523| 2.114755425979916|2079.968670461382|15109.15257531584|1.4941690962099126|0.007543153315748068|0.23420796890184645|3.4094590217039196|7.656717108612153|1788.4380582164838|291.53061224489795|1971.001295756398|84.32625295015966|98077.93928455736|   47.5600489471979| -122.2138924984964|1986.5972048683418|12769.886852700263|540100.4469434032|\n",
      "| stddev|0.4676036229469256|0.9300844679400377|0.7702250380496175|918.4856621619584|  41424.011832657| 0.539908826913613| 0.08652514438777897| 0.7661525314655753|0.6507729923679282| 1.17549188408608| 828.1290214139121| 442.6067613434852|29.37149229430725|401.5081449287515|53.50788259398413|0.13857485395942823|0.14082719264858765| 685.4087217757766| 27306.39442223669|367156.2021973736|\n",
      "|    min|              2014|                 0|               0.0|              290|              520|               1.0|                   0|                  0|                 1|                1|               290|                 0|             1900|                0|            98001|            47.1559|           -122.519|               399|               651|          75000.0|\n",
      "|    max|              2015|                33|               8.0|            13540|          1651359|               3.5|                   1|                  4|                 5|               13|              9410|              4820|             2015|             2015|            98199|            47.7776|           -121.315|              6210|            871200|        7700000.0|\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+------------------+--------------------+-------------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+-----------------+-------------------+-------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check and drop duplicate rows\n",
    "print(\"Duplicate rows before:\", df.count() - df.distinct().count()) # Check duplicate rows before dropping\n",
    "\n",
    "# Drop duplicate rows\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "print(\"Duplicate rows after:\", df.count() - df.distinct().count()) # Check duplicate rows after dropping\n",
    "\n",
    "# statistical summary after dropping duplicates \n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before removing the duplicates, there were 21,613 rows of data but after, there are data 21,609 meaning there were 4 of duplicate data as shown in the code snippet below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming Features\n",
    "\n",
    "The MLlib algorithms expect the data to be represented in two columns: Features and Label. Features are an array of data points of all the features to be used for prediction while the Label contain the output label for each data point.\n",
    "\n",
    "Before building the model, we will assemble the input features into a single feature vector using the VectorAssembler class, transform the data and then select the features and the label (price).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a VectorAssembler for a single feature\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"zipcode\", \"yr_sold\", \"long\", \"condition\", \"yr_built\", \"sqft_lot15\", \"sqft_lot\", \"yr_renovated\", \"floors\", \n",
    "               \"waterfront\", \"lat\", \"bedrooms\", \"sqft_basement\", \"view\", \"bathrooms\", \"sqft_living15\", \"sqft_above\", \"grade\",\n",
    "                 \"sqft_living\"], \n",
    "                 outputCol=\"feature\")\n",
    "\n",
    "# Transform the data\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Select feature and label (predictor and target features)\n",
    "df_final = df.select(\"feature\", \"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the data into a training set (70%) and a testing set (30%) using randomSplit() function to select random rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "+--------------------+--------+\n",
      "|             feature|   price|\n",
      "+--------------------+--------+\n",
      "|[98001.0,2014.0,-...|290000.0|\n",
      "|[98001.0,2014.0,-...|420000.0|\n",
      "|[98001.0,2014.0,-...|214100.0|\n",
      "|[98001.0,2014.0,-...|335000.0|\n",
      "|[98001.0,2014.0,-...|199900.0|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test data:\n",
      "+--------------------+--------+\n",
      "|             feature|   price|\n",
      "+--------------------+--------+\n",
      "|[98001.0,2014.0,-...|850000.0|\n",
      "|[98001.0,2014.0,-...|571500.0|\n",
      "|[98001.0,2014.0,-...|310000.0|\n",
      "|[98001.0,2014.0,-...|280000.0|\n",
      "|[98001.0,2014.0,-...|185000.0|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we will split the data into a training set (70%) and a testing set (30%)\n",
    "df_train, df_test = df_final.randomSplit([0.7, 0.3])\n",
    "print('Train data:')\n",
    "df_train.show(5)\n",
    "\n",
    "print('Test data:')\n",
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELS DEVELOPMENT\n",
    "\n",
    "Next, let us create instances of the regression models we are interested in and fit the models to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object of the LinearRegression model\n",
    "lr = LinearRegression(featuresCol='feature', labelCol='price')\n",
    "\n",
    "# create an object of the DecisionTreeRegressor model\n",
    "dt = DecisionTreeRegressor(featuresCol='feature', labelCol='price')\n",
    "\n",
    "# create an object of the RandomForestRegressor algorithm\n",
    "rf = RandomForestRegressor(featuresCol='feature', labelCol='price', numTrees=2, maxDepth=2, seed=42)\n",
    "\n",
    "# fit the models to the training data.\n",
    "model = lr.fit(df_train)\n",
    "model_2 = dt.fit(df_train)\n",
    "model_3 = rf.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICTION AND EVALUATION\n",
    "\n",
    "We will use the trained models to make predictions on the test data and evaluate each model's performance to identify the best model. \n",
    "\n",
    "There are different model evaluation metrics, to calculate the accuracy of each of the model, will be using the R-squared (R2) and Root Mean Squared Error (RMSE) metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "predictions = model.transform(df_test)\n",
    "predictions_2 = model_2.transform(df_test)\n",
    "predictions_3 = model_3.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Model:\n",
      "R2 Score: 0.71\n",
      "Root Mean Squared Error (RMSE): 186479.27\n",
      "\n",
      "DecisionTreeRegressor Model:\n",
      "R2 Score: 0.72\n",
      "Root Mean Squared Error (RMSE): 182072.83\n",
      "\n",
      "RandomForestRegressor:\n",
      "R2 Score: 0.53\n",
      "Root Mean Squared Error (RMSE): 237136.10\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "print(\"Linear Regression Model:\")\n",
    "# R2 score\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(\"R2 Score: {:.2f}\".format(r2))\n",
    "# root mean squared error \n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "\n",
    "# DecisionTreeRegressor\n",
    "print('')\n",
    "print(\"DecisionTreeRegressor Model:\")\n",
    "# R2 score\n",
    "evaluator_r2_2 = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2_2 = evaluator_r2_2.evaluate(predictions_2)\n",
    "print(\"R2 Score: {:.2f}\".format(r2_2))\n",
    "# root mean squared error \n",
    "evaluator_rmse_2 = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_2 = evaluator_rmse.evaluate(predictions_2)\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse_2))\n",
    "\n",
    "# RandomForestRegressor\n",
    "print('')\n",
    "print(\"RandomForestRegressor:\")\n",
    "# R2 score\n",
    "evaluator_r2_3 = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2_3 = evaluator_r2_3.evaluate(predictions_3)\n",
    "print(\"R2 Score: {:.2f}\".format(r2_3))\n",
    "# root mean squared error\n",
    "evaluator_rmse_3 = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_3 = evaluator_rmse.evaluate(predictions_3)\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moedel 1: Linear Regression Model\n",
    "\n",
    "With a R2 Score of 0.71 and RMSE of 186,479.27, this model explains 71% of the variance in the data and the RMSE seems moderate, indicating some prediction error.\n",
    "\n",
    "Model 2: DecisionTreeRegressor Model\n",
    "\n",
    "With a R2 Score of 0.72 and RMSE of 182,072.83, this model slightly outperforms the Linear Regression model in explaining the variance. Additionally, it has the lowest RMSE among the models, suggesting it provides the most accurate predictions overall.\n",
    "\n",
    "Model 3: RandomForestRegressor Model\n",
    "\n",
    "With a R2 Score of 0.53 and RMSE of 237,136.10, this model explains only 53% of the variance and has the highest RMSE, indicating the least accurate predictions among the three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION\n",
    "\n",
    "Comparing these three models, we conclude that the DecisionTreeRegressor Model is the best model to be able to predict price from our dataset. it has the highest R2 score (0.72) and the lowest RMSE (182,072.83), balanceing variance explanation with predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
